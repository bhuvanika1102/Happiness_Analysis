{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pre-requisite libraries\n",
    "\n",
    "import pandas as pd # For dealing with dataframes (i.e. pd.read_csv())\n",
    "import matplotlib.pyplot as plt # Displaying graphs \n",
    "from sklearn.model_selection import train_test_split # Seperating the training and testing process\n",
    "from sklearn.impute import SimpleImputer # For replacing missing values\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline # Easier preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor # Will be used for the model\n",
    "from sklearn.preprocessing import OrdinalEncoder # For dealing with categorical values\n",
    "from sklearn.metrics import mean_absolute_error # MAE for model accuracy determination\n",
    "from xgboost import XGBRegressor # Alternative model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf8f98",
   "metadata": {},
   "source": [
    "## Preprocessing of data + Training + Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb568354",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Ladder score'] # Target data (happiness)\n",
    "\n",
    "data_features = ['Country name', 'Regional indicator', 'Freedom to make life choices', \n",
    "                 'Perceptions of corruption'] # Features for training + testing\n",
    "\n",
    "X = data[data_features]\n",
    "\n",
    "# Dividing data into training and validation subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8,\n",
    "                                                     test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd18bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for categorical variables\n",
    "\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\", object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91983e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the training and validation sets to fit the encoder\n",
    "combined_X = pd.concat([X_train[object_cols], X_valid[object_cols]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c15b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal Encoding categorical variables\n",
    "\n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Applying ordinal encoding to categorical variables\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "ordinal_encoder.fit(combined_X)\n",
    "label_X_train[object_cols] = ordinal_encoder.transform(X_train[object_cols])\n",
    "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dee628",
   "metadata": {},
   "source": [
    "### Dealing with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db0383",
   "metadata": {},
   "source": [
    "#### Imputing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae89f305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values\n",
    "my_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(label_X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(label_X_valid))\n",
    "\n",
    "# Restore column names after imputation\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b08a56",
   "metadata": {},
   "source": [
    "#### Removing null values (alternative approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12909d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing null-values from the dataset\n",
    "\n",
    "missing_cols = [col for col in X_train.columns\n",
    "               if X_train[col].isnull().any()] # Count null values present\n",
    "\n",
    "# Drop columns in both training and validation data\n",
    "reduced_X_train = label_X_train.drop(missing_cols, axis=1)\n",
    "reduced_X_valid = label_X_valid.drop(missing_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19605545",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor (Training + Testing)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b1621d5",
   "metadata": {},
   "source": [
    "# MAE function (return the MAE for the given model)\n",
    "\n",
    "def getMAE(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=500, random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(reduced_X_valid) # Predictions with the model\n",
    "    return mean_absolute_error(y_valid, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1459e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for imputed values\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=150, random_state=1)\n",
    "model.fit(imputed_X_train, y_train)\n",
    "predictions = model.predict(imputed_X_valid) # Predictions with the model\n",
    "\n",
    "print(\"MAE from Approach 1 (Imputed Values):\")\n",
    "print(mean_absolute_error(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d51e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for dropped values\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=500, random_state=1)\n",
    "model.fit(reduced_X_train, y_train)\n",
    "predictions = model.predict(reduced_X_valid) # Predictions with the model\n",
    "\n",
    "print(\"MAE from Approach 2 (Dropped Values):\")\n",
    "print(mean_absolute_error(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae34ff",
   "metadata": {},
   "source": [
    "#### XGBoost + Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use imputed values, as they've provided the best accuracy\n",
    "\n",
    "my_model = XGBRegressor(n_estimators=500, learning_rate=0.85, random_state=1)\n",
    "my_model.fit(imputed_X_train, y_train,\n",
    "             early_stopping_rounds=5,\n",
    "             eval_set=[(imputed_X_valid, y_valid)],\n",
    "             verbose=False)\n",
    "\n",
    "pred = my_model.predict(imputed_X_valid)\n",
    "print(\"MAE from Approach 3 (XGB Regressor + Imputation): \")\n",
    "print(mean_absolute_error(y_valid, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
